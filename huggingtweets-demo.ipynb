{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "huggingtweets-demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHAWMW78AG9",
        "colab_type": "text"
      },
      "source": [
        "# HuggingTweets - Train a model to generate tweets\n",
        "\n",
        "Coming up with original tweets is hard â€” train a neural network to do it for you! Put in your favorite Twitter handle, and train a model in to write new tweets based on their unique voice, in just 5 minutes.\n",
        "\n",
        "Here's an example where I fine-truned the neural network to predict Elon Musk's next breakthrough ;)\n",
        "\n",
        "![huggingtweets illustration](https://i.imgur.com/MXKx10d.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwj6GL6LJvDI",
        "colab_type": "text"
      },
      "source": [
        "## To start the demo, click on menu at top, \"Runtime\" â†’ \"Run all\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ZSCf6QyF8AG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "\n",
        "def print_html(x):\n",
        "    \"Better printing\"\n",
        "    x = x.replace('\\n', '<br>')\n",
        "    display(HTML(x))\n",
        "        \n",
        "# Check we use GPU\n",
        "import torch\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "if not torch.cuda.is_available():\n",
        "    print_html('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
        "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
        "    raise ValueError('No GPU available')\n",
        "else:\n",
        "    # Install dependencies\n",
        "    !pip install wandb transformers torch -qq\n",
        "\n",
        "    import ipywidgets as widgets\n",
        "    from IPython import get_ipython\n",
        "    import json\n",
        "    import urllib3\n",
        "    import random\n",
        "    import wandb\n",
        "    wandb.login(anonymous='allow')  # ensure we log with huggingface\n",
        "    from transformers import (\n",
        "        AutoTokenizer, AutoModelForCausalLM,\n",
        "        TextDataset, DataCollatorForLanguageModeling,\n",
        "        Trainer, TrainingArguments)\n",
        "    \n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "    \n",
        "    log_debug = widgets.Output()\n",
        "\n",
        "    def fix_text(text):\n",
        "        text = text.replace('&amp;', '&')\n",
        "        text = text.replace('&lt;', '<')\n",
        "        text = text.replace('&gt;', '>')\n",
        "        return text\n",
        "\n",
        "    def html_table(data, title=None):\n",
        "        'Create a html table'\n",
        "        width_twitter = '75px'\n",
        "        def html_cell(i, twitter_button=False):\n",
        "            return f'<td style=\"width:{width_twitter}\">{i}</td>' if twitter_button else f'<td>{i}</td>'\n",
        "        def html_row(row):\n",
        "            return f'<tr>{\"\".join(html_cell(r, not i if len(row)>1 else False) for i,r in enumerate(row))}</tr>'    \n",
        "        body = f'<table style=\"width:100%\">{\"\".join(html_row(r) for r in data)}</table>'\n",
        "        title_html = f'<h3>{title}</h3>' if title else ''\n",
        "        html = '''\n",
        "        <html>\n",
        "            <head>\n",
        "                <style>\n",
        "                    table {border-collapse: collapse !important;}\n",
        "                    td {text-align:left !important; border: solid #E3F2FD !important; border-width: 1px 0 !important; padding: 6px !important;}\n",
        "                    tr:nth-child(even) {background-color: #E3F2FD !important;}\n",
        "                </style>\n",
        "            </head>\n",
        "            <body>''' + title_html + body + '</body></html>'\n",
        "        return(html)\n",
        "\n",
        "    def clean_tweet(tweet):\n",
        "        bad_start = ['http', 'pic']\n",
        "        text = ' '.join(t for t in tweet.split() if not(any(bs in t for bs in bad_start)))\n",
        "        return text\n",
        "        \n",
        "    def boring_tweet(tweet):\n",
        "        \"Check if this is a boring tweet\"\n",
        "        boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
        "        if len(tweet.split()) < 3:\n",
        "            return True\n",
        "        if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet.split()):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def dl_tweets():\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_dl_tweets.button_style = 'primary'\n",
        "\n",
        "        get_ipython().kernel.do_one_iteration() # widget slow to update on phones\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower()\n",
        "        log_dl_tweets.clear_output(wait=True)\n",
        "\n",
        "        try_success = False\n",
        "\n",
        "        with log_dl_tweets:\n",
        "            try:\n",
        "                print_html(f'\\nDownloading {handle_widget.value.strip()} tweets... This should take no more than a minute!')\n",
        "                http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "                res = http.request(\"GET\", f\"http://us-central1-playground-111.cloudfunctions.net/tweets_http?handle={handle}\")\n",
        "                curated_tweets = json.loads(res.data.decode('utf-8'))\n",
        "                curated_tweets = [fix_text(tweet) for tweet in curated_tweets]\n",
        "                log_dl_tweets.clear_output(wait=True)\n",
        "                print_html(f'\\n{len(curated_tweets)} tweets from {handle_widget.value.strip()} downloaded!\\n\\n')\n",
        "                    \n",
        "                # create dataset\n",
        "                clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "                cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
        "                total_text = '<|endoftext|> ' + ' <|endoftext|> '.join(cool_tweets) + ' <|endoftext|>'\n",
        "                \n",
        "                # display a few tweets\n",
        "                random.shuffle(curated_tweets)\n",
        "                example_tweets = [[t] for  t in curated_tweets[-8:]]\n",
        "                display(HTML(html_table(example_tweets)))\n",
        "\n",
        "                if len(total_text) < 6000:\n",
        "                    # need about 4000 chars for one data sample (but depends on spaces, etc)\n",
        "                    raise ValueError('Error: this user does not have enough tweets to train a Neural Network')\n",
        "\n",
        "                if len(total_text) < 40000:\n",
        "                    print_html('\\n<b>Warning: this user does not have many tweets which may impact the results of the Neural Network</b>')\n",
        "\n",
        "                with open(f'data_{handle}_train.txt', 'w') as f:\n",
        "                    f.write(total_text)\n",
        "                \n",
        "                run_dl_tweets.button_style = 'success'\n",
        "                log_finetune.clear_output(wait=True)\n",
        "                run_finetune.disabled = False\n",
        "\n",
        "                try_success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_dl_tweets.button_style = 'danger'\n",
        "        \n",
        "        if try_success:\n",
        "            log_finetune.clear_output(wait=True)\n",
        "            with log_finetune:\n",
        "                print_html('\\nFine-tune your model by clicking on \"Train Neural Network\"')\n",
        "                \n",
        "        handle_widget.disabled = False\n",
        "        run_dl_tweets.disabled = False\n",
        "                \n",
        "    handle_widget = widgets.Text(value='@elonmusk',\n",
        "                                placeholder='Enter twitter handle')\n",
        "\n",
        "    run_dl_tweets = widgets.Button(\n",
        "        description='Download tweets',\n",
        "        button_style='primary')\n",
        "    def on_run_dl_tweets_clicked(b):\n",
        "        dl_tweets()\n",
        "    run_dl_tweets.on_click(on_run_dl_tweets_clicked)\n",
        "\n",
        "    log_restart = widgets.Output()\n",
        "    log_dl_tweets = widgets.Output()\n",
        "        \n",
        "    # Associate run to a project\n",
        "    with log_debug:\n",
        "        %env WANDB_PROJECT=huggingtweets\n",
        "        %env WANDB_WATCH=false\n",
        "        %env WANDB_ENTITY=wandb\n",
        "        %env WANDB_ANONYMOUS=allow\n",
        "        %env WANDB_NOTEBOOK_NAME=huggingtweets-demo\n",
        "        %env WANDB_RESUME=allow\n",
        "        %env WANDB_NOTES=Github repo: https://github.com/borisdayma/huggingtweets\n",
        "\n",
        "    # Have global access to model & tokenizer\n",
        "    trainer, tokenizer = None, None\n",
        "    \n",
        "    def finetune():\n",
        "        if run_finetune.button_style == 'success':\n",
        "            # user double clicked before start of function\n",
        "            return\n",
        "\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_finetune.disabled = True\n",
        "        run_finetune.button_style = 'primary'\n",
        "\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower()\n",
        "        log_finetune.clear_output(wait=True)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        with log_finetune:\n",
        "            print_html(f'\\nTraining Neural Network on {handle_widget.value.strip()} tweets... This could take up to 2-3 minutes!\\n')\n",
        "            progress = widgets.FloatProgress(value=0.1, min=0.0, max=1.0, bar_style = 'info')\n",
        "            display(progress)\n",
        "\n",
        "        with log_debug:\n",
        "            try:\n",
        "                # use new run id\n",
        "                run_id = wandb.util.generate_id()\n",
        "                %env WANDB_RUN_ID=$run_id\n",
        "                run_name = handle_widget.value.strip()\n",
        "                %env WANDB_NAME=$run_name\n",
        "                wandb.init(config={'version':0.2})\n",
        "                \n",
        "                # Setting up pre-trained neural network\n",
        "                with log_finetune:\n",
        "                    print_html('\\nSetting up pre-trained neural network...')\n",
        "                global trainer, tokenizer\n",
        "                tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "                model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "                block_size = tokenizer.max_len\n",
        "                train_dataset = TextDataset(tokenizer=tokenizer, file_path=f'data_{handle}_train.txt', block_size=block_size, overwrite_cache=True)\n",
        "                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "                epochs = 4  # limit before overfitting\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=f'output/{handle}',\n",
        "                    overwrite_output_dir=True,\n",
        "                    do_train=True,\n",
        "                    num_train_epochs=epochs,\n",
        "                    per_device_train_batch_size=1,\n",
        "                    logging_steps=5,\n",
        "                    save_steps=0,\n",
        "                    seed=random.randint(0,2**32-1))\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=training_args,\n",
        "                    data_collator=data_collator,\n",
        "                    train_dataset=train_dataset,\n",
        "                    prediction_loss_only=True)\n",
        "                progress.value = 0.4\n",
        "                \n",
        "                p_start, p_end = 0.4, 1.\n",
        "                def progressify(f):\n",
        "                    \"Control progress bar when calling f\"\n",
        "                    def inner(*args, **kwargs):\n",
        "                        if trainer.epoch is not None:\n",
        "                            progress.value = p_start + trainer.epoch / epochs * (p_end - p_start)\n",
        "                        return f(*args, **kwargs)\n",
        "                    return inner\n",
        "        \n",
        "                trainer._training_step = progressify(trainer._training_step)\n",
        "                \n",
        "                # Training neural network\n",
        "                with log_finetune:\n",
        "                    print_html('Training neural network...\\n')\n",
        "                    display(wandb.jupyter.Run())\n",
        "                    print_html('\\n')\n",
        "                    display(progress)\n",
        "                trainer.train()\n",
        "\n",
        "                run_finetune.button_style = 'success'\n",
        "                run_predictions.disabled = False\n",
        "\n",
        "                progress.value = 1.0\n",
        "                progress.bar_style = 'success'\n",
        "                success_try = True\n",
        "\n",
        "                with log_finetune:\n",
        "                    print_html('\\nðŸŽ‰ Neural network trained successfully!')\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                with log_predictions:\n",
        "                    print_html('\\nEnter the start of a sentence and click \"Run predictions\"')\n",
        "                with log_restart:\n",
        "                    print_html('\\n<b>To change user, click on menu \"Runtime\" â†’ \"Restart and run all\"</b>\\n')\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_finetune.button_style = 'danger'\n",
        "                run_finetune.disabled = False\n",
        "                            \n",
        "        if not success_try:\n",
        "            display(log_debug)\n",
        "            progress.bar_style = 'danger'\n",
        "        \n",
        "    run_finetune = widgets.Button(\n",
        "        description='Train Neural Network',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_finetune_clicked(b):\n",
        "        finetune()\n",
        "    run_finetune.on_click(on_run_finetune_clicked)\n",
        "\n",
        "    log_finetune = widgets.Output()\n",
        "    with log_finetune:\n",
        "        print_html('\\nWaiting for Step 1 to complete...')\n",
        "\n",
        "    def clean_prediction(text):\n",
        "        token = '<|endoftext|>'\n",
        "        text = text.replace(token, '')\n",
        "        text = text.strip()\n",
        "        if text[-1] == '\"' and text.count('\"') % 2: text = text[:-1]\n",
        "        return text.strip()\n",
        "\n",
        "    predictions = []\n",
        "    \n",
        "    def shorten_text(text, max_char):\n",
        "        while len(text) > max_char:\n",
        "            text = ' '.join(text.split()[:-1]) + 'â€¦'\n",
        "        return text\n",
        "        \n",
        "    def predict():\n",
        "        run_predictions.disabled = True\n",
        "        start_widget.disabled = True\n",
        "        run_predictions.button_style = 'primary'\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle_uncased = handle\n",
        "        handle = handle.lower()\n",
        "        log_predictions.clear_output(wait=True)\n",
        "\n",
        "        # tweet buttons don't appear well in colab if within log_predictions widget\n",
        "        # we reset the entire cell\n",
        "        clear_output(wait=True)\n",
        "        display(widgets.VBox([start_widget, run_predictions, log_predictions]))\n",
        "\n",
        "        def tweet_html(text):\n",
        "            tweet_text = f'Trained a neural network on @{handle_uncased}: {start} â†’ {text}'\n",
        "            tweet_text = shorten_text(tweet_text, 238)\n",
        "            tweet_text = tweet_text.replace('\"', '&quot;')\n",
        "\n",
        "            return '<a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" data-size=\"large\" '\\\n",
        "                    f'data-text=\"{tweet_text}\" '\\\n",
        "                    f'data-url=\"{wandb_url}\" data-hashtags=\"huggingtweets\" data-related=\"borisdayma,weights_biases,huggingface\"'\\\n",
        "                    'data-show-count=\"false\">Tweet</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>'\n",
        "        \n",
        "        success_try = False\n",
        "\n",
        "        # get start sentence\n",
        "        get_ipython().kernel.do_one_iteration() # widget slow to update on phones\n",
        "        start = start_widget.value.strip()\n",
        "                \n",
        "        with log_predictions:\n",
        "            print_html(f'\\nPerforming predictions of @{handle} starting with \"{start}\"...\\nThis should take no more than 10 seconds!')\n",
        "        \n",
        "        with log_debug:\n",
        "            try:                \n",
        "                # start a wandb run (should never happen)\n",
        "                if wandb.run is None:\n",
        "                    run_name = handle_widget.value.strip()\n",
        "                    %env WANDB_NAME=$run_name\n",
        "                    wandb.init()\n",
        "\n",
        "                wandb_url = wandb.run.get_url()\n",
        "                \n",
        "                # prepare input\n",
        "                start_with_bos = '<|endoftext|> ' + start\n",
        "                encoded_prompt = tokenizer.encode(start_with_bos, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
        "\n",
        "                # prediction\n",
        "                output_sequences = trainer.model.generate(\n",
        "                    input_ids=encoded_prompt,\n",
        "                    max_length=160,\n",
        "                    min_length=10,\n",
        "                    temperature=1.,\n",
        "                    top_p=0.95,\n",
        "                    do_sample=True,\n",
        "                    num_return_sequences=20\n",
        "                    )\n",
        "                stop_token = '<|endoftext|>'\n",
        "                generated_sequences = []\n",
        "\n",
        "                # decode prediction\n",
        "                for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "                    generated_sequence = generated_sequence.tolist()\n",
        "                    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "                    limit = text.find(stop_token, 1)\n",
        "                    text = text[: limit if limit != -1 else None]\n",
        "                    # we also remove new lines which happen due to pre-trained gpt-2 and twitter scraping\n",
        "                    limit = text.find('\\n')\n",
        "                    text = text[: limit if limit != -1 else None]\n",
        "                    generated_sequence = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "                    generated_sequences.append(clean_prediction(generated_sequence))\n",
        "                \n",
        "                for i, g in enumerate(generated_sequences):\n",
        "                    predictions.append([start, ' '.join([start, g])])\n",
        "                \n",
        "                # log predictions\n",
        "                wandb.log({'examples': wandb.Table(data=predictions, columns=['Input', 'Prediction'])})\n",
        "\n",
        "                # make html table\n",
        "                tweet_data = [[tweet_html(g), ' '.join([start, g])] for g in generated_sequences]\n",
        "                tweet_table = HTML(html_table(tweet_data))\n",
        "\n",
        "                run_predictions.button_style = 'success'\n",
        "                success_try = True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_predictions.button_style = 'danger'\n",
        "\n",
        "        if success_try:\n",
        "            with log_predictions:\n",
        "                log_predictions.clear_output(wait=True)\n",
        "\n",
        "                # display wandb run\n",
        "                link = f'<a href=\"{wandb_url}\" rel=\"noopener\" target=\"_blank\">{wandb_url}</a>'\n",
        "                print_html(f'\\nðŸš€ View all results under the \"Media\" panel at {link}\\n')\n",
        "                print_html('\\n<b>Click on your favorite tweet or try new predictions with other sentences (or the same one)!</b>\\n\\n')\n",
        "                \n",
        "                # somehow display works one way with Jupyter and one way with colab\n",
        "                if not IN_COLAB:\n",
        "                    display(tweet_table)\n",
        "                    print_html('\\n<b>Click on your favorite tweet or try new predictions with other sentences (or the same one)!</b>\\n\\n')\n",
        "            if IN_COLAB:\n",
        "                display(tweet_table)\n",
        "                print_html('\\n<b>Click on your favorite tweet or try new predictions with other sentences (or the same one)!</b>\\n\\n')\n",
        "        else:\n",
        "            display(log_debug)\n",
        "        \n",
        "        run_predictions.disabled = False\n",
        "        start_widget.disabled = False\n",
        "                \n",
        "    start_widget = widgets.Text(value='My dream is',\n",
        "                                placeholder='Start a sentence')\n",
        "\n",
        "    run_predictions = widgets.Button(\n",
        "        description='Run predictions',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_predictions_clicked(b):\n",
        "        predict()\n",
        "    run_predictions.on_click(on_run_predictions_clicked)\n",
        "\n",
        "    log_predictions = widgets.Output()\n",
        "    with log_predictions:\n",
        "        print_html('\\nWaiting for Step 2 to complete...')\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print_html(\"ðŸŽ‰ Environment set-up correctly! You're ready to move to Step 1!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpSPr0T8AHD",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Enter a Twitter handle\n",
        "\n",
        "Enter a Twitter handle and click Download tweets. This gives the model a dataset of examples to train on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "6O-8Kr_m8AHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "display(widgets.VBox([handle_widget, run_dl_tweets, log_restart, log_dl_tweets]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_ArgCZ8AHH",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Train your Neural Network\n",
        "\n",
        "Fine-tune a language model on your unique set of tweets to generate predictions. The model is based on [HuggingFace](https://huggingface.co/), an awesome open source library for Natural Language Processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "CpxBQYF88AHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "display(widgets.VBox([run_finetune, log_finetune]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfRPh4V18AHM",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Generate tweets\n",
        "\n",
        "Type the beginning of a tweet, press Run predictions, and the model will try to come up with a realistic ending to your tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FlMACi0-8AHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "if IN_COLAB:\n",
        "    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 2000})'''))\n",
        "display(widgets.VBox([start_widget, run_predictions, log_predictions]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlRI2hsBKtz6",
        "colab_type": "text"
      },
      "source": [
        "Huggingtweets is still in its infancy and will get better over time!\n",
        "\n",
        "In the future, it will train continuously to become a Twitter expert!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iawnVxrItM",
        "colab_type": "text"
      },
      "source": [
        "## About\n",
        "\n",
        "*Built by Boris Dayma*\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/borisdayma)\n",
        "\n",
        "My main goals with this project are:\n",
        "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
        "* to make AI accessible to everyone.\n",
        "\n",
        "To see how the model works, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
        "\n",
        "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtGe95IHKkyS",
        "colab_type": "text"
      },
      "source": [
        "## Resources\n",
        "\n",
        "### [Explore the live report â†’](https://bit.ly/2TGXMZf)\n",
        "* [A Step by Step Guide to Tracking Hugging Face Model Performance](https://app.wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU)\n",
        "* [W&B Forum](http://bit.ly/wandb-forum): If you have any questions, reach out to the slack community"
      ]
    }
  ]
}