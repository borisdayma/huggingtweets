{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "huggingtweets-demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHAWMW78AG9"
      },
      "source": [
        "# HuggingTweets - Train a model to generate tweets\n",
        "\n",
        "Choose your favorite Twitter account and train a language model to write new tweets based on their unique voice in just 5 minutes.\n",
        "\n",
        "Here is an example where I fine-tuned a neural network to predict Elon Musk's next breakthrough ðŸ˜‰\n",
        "\n",
        "![huggingtweets illustration](https://raw.githubusercontent.com/borisdayma/huggingtweets/master/img/example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwj6GL6LJvDI"
      },
      "source": [
        "## To start the demo, click on menu at top, \"Runtime\" â†’ \"Run all\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ZSCf6QyF8AG-"
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "\n",
        "def stylize():\n",
        "    \"Handle dark mode\"\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "    :root {\n",
        "        --table_bg: #EBF8FF;\n",
        "    }\n",
        "    html[theme=dark] {\n",
        "        --colab-primary-text-color: #d5d5d5;\n",
        "        --table_bg: #2A4365;\n",
        "    }\n",
        "    .jupyter-widgets {\n",
        "        color: var(--colab-primary-text-color);\n",
        "    }\n",
        "    table {\n",
        "        border-collapse: collapse !important;\n",
        "    }\n",
        "    td {\n",
        "        text-align:left !important;\n",
        "        border: solid var(--table_bg) !important;\n",
        "        border-width: 1px 0 !important;\n",
        "        padding: 6px !important;\n",
        "    }\n",
        "    tr:nth-child(even) {\n",
        "        background-color: var(--table_bg) !important;\n",
        "    }\n",
        "    .table_odd {\n",
        "        background-color: var(--table_bg) !important;\n",
        "        margin: 0 !important;\n",
        "    }\n",
        "    .table_even {\n",
        "        border: solid var(--table_bg) !important;\n",
        "        border-width: 1px 0 !important;\n",
        "        margin: 0 !important;\n",
        "    }\n",
        "    .jupyter-widgets {\n",
        "        margin: 6px;\n",
        "    }\n",
        "    .widget-html-content {\n",
        "        font-size: var(--colab-chrome-font-size) !important;\n",
        "        line-height: 1.24 !important;\n",
        "    }\n",
        "    </style>'''))\n",
        "\n",
        "def print_html(x):\n",
        "    \"Better printing\"\n",
        "    x = x.replace('\\n', '<br>')\n",
        "    display(HTML(x))\n",
        "        \n",
        "# Check we use GPU\n",
        "import torch\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "if not torch.cuda.is_available():\n",
        "    print_html('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
        "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
        "    raise ValueError('No GPU available')\n",
        "else:\n",
        "    # colab requires special handling\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    # Install dependencies (mainly for colab)\n",
        "    if IN_COLAB:\n",
        "        !pip install git+https://github.com/huggingface/transformers\n",
        "        !pip install torch wandb -qq\n",
        "        !sudo apt-get install git-lfs\n",
        "\n",
        "    import ipywidgets as widgets\n",
        "    from IPython import get_ipython\n",
        "    import json\n",
        "    import urllib3\n",
        "    import pathlib\n",
        "    import shutil\n",
        "    import requests\n",
        "    import os\n",
        "    import re\n",
        "    import random\n",
        "    import wandb\n",
        "    \n",
        "    stylize()\n",
        "    \n",
        "    log_debug = widgets.Output()\n",
        "    \n",
        "    # Have global access\n",
        "    trainer = None\n",
        "    artifact_dataset = None\n",
        "    metadata = {}\n",
        "    card_val = {}\n",
        "    model_preview = None\n",
        "    hfapi, token, namespace = None, None, None\n",
        "    handle = ''\n",
        "\n",
        "    # W&B variables\n",
        "    WANDB_PROJECT = 'huggingtweets'\n",
        "    WANDB_NOTES = \"Github repo: https://github.com/borisdayma/huggingtweets\"\n",
        "    WANDB_ENTITY = 'wandb'\n",
        "    HW_VERSION = 0.4\n",
        "    os.environ['WANDB_NOTEBOOK_NAME'] = 'huggingtweets-demo.ipynb'  # used in wandb cli\n",
        "\n",
        "    # HYPER-PARAMETERS\n",
        "    ALLOW_NEW_LINES = False     # seems to work better\n",
        "    LEARNING_RATE = 1.372e-4\n",
        "    EPOCHS = 4\n",
        "\n",
        "    def fix_text(text):\n",
        "        text = text.replace('&amp;', '&')\n",
        "        text = text.replace('&lt;', '<')\n",
        "        text = text.replace('&gt;', '>')\n",
        "        return text\n",
        "\n",
        "    def html_table(data, title=None):\n",
        "        'Create a html table'\n",
        "        width_twitter = '75px'\n",
        "        def html_cell(i, twitter_button=False):\n",
        "            nl = \"\\n\"\n",
        "            return f'<td style=\"width:{width_twitter}\">{i}</td>' if twitter_button else f'<td>{i.replace(nl, \"<br>\")}</td>'\n",
        "        def html_row(row):\n",
        "            return f'<tr>{\"\".join(html_cell(r, not i if len(row)>1 else False) for i,r in enumerate(row))}</tr>'\n",
        "        body = f'<table style=\"width:100%\">{\"\".join(html_row(r) for r in data)}</table>'\n",
        "        title_html = f'<h3>{title}</h3>' if title else ''\n",
        "        html = '<html><body>' + title_html + body + '</body></html>'\n",
        "        return(html)\n",
        "\n",
        "    def clean_tweet(tweet, allow_new_lines = ALLOW_NEW_LINES):\n",
        "        bad_start = ['http:', 'https:']\n",
        "        for w in bad_start:\n",
        "            tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
        "            tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
        "            tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
        "        tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space\n",
        "        if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
        "            tweet = ' '.join(tweet.split())\n",
        "        return tweet.strip()\n",
        "        \n",
        "    def boring_tweet(tweet):\n",
        "        \"Check if this is a boring tweet\"\n",
        "        boring_stuff = ['http', '@', '#']\n",
        "        not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
        "        return not_boring_words < 3\n",
        "\n",
        "    def create_model_card(card_val, output_dir):\n",
        "        model_card_url = 'https://github.com/borisdayma/huggingtweets/raw/master/model_card/README.md'\n",
        "        model_card = requests.get(model_card_url).content.decode('utf-8')\n",
        "        for k, v in card_val.items():\n",
        "            model_card = model_card.replace(k, v)\n",
        "        # make model card size unique (for correct sync in huggingface)\n",
        "        model_card = model_card.replace(\"RANDOM_SZ\", ' ' * random.randint(1, 100))\n",
        "        with open(f'{output_dir}/README.md', 'w') as f:\n",
        "            f.write(model_card)\n",
        "    \n",
        "    def on_preview_clicked(b):\n",
        "        global model_preview\n",
        "        global hfapi, token, namespace\n",
        "        model_preview = f'https://www.huggingtweets.com/{handle}/{b.url_id}/predictions.png'\n",
        "        card_val['SOCIAL_LINK'] = model_preview\n",
        "        create_model_card(card_val, handle)\n",
        "        readme = pathlib.Path(handle) / 'README.md'\n",
        "        readme_path, readme_name = str(readme.resolve()), str(readme)\n",
        "        hfapi.presign_and_upload(token, filename=readme_name, filepath=readme_path, organization=namespace)\n",
        "\n",
        "        # Reset view\n",
        "        log_model.clear_output(wait=True)\n",
        "        with log_model:\n",
        "            print_html(\"<h2>Model Preview (select a tweet to update)</h2>\")\n",
        "            display(HTML(f'<img src=\"{model_preview}\" width=560 style=\"border: 1px solid lightgray; margin:5px;\">'))\n",
        "        \n",
        "    def create_button(url_id):\n",
        "        layout = widgets.Layout(width='70px', min_width='70px') #set width and height\n",
        "        button = widgets.Button(\n",
        "            description='Preview',\n",
        "            button_style='info',\n",
        "            layout = layout,\n",
        "            tooltip = 'Set as model preview'\n",
        "        )\n",
        "        button.url_id = url_id\n",
        "        button.on_click(on_preview_clicked)\n",
        "        return button\n",
        "\n",
        "    def ensure_widgets_updated(n_iter=5):\n",
        "        '''ensure we get correct inputs and states are updated'''\n",
        "        pass\n",
        "        # used to be necessary in colab ; seems not needed anymore and create issues like in Jupyter\n",
        "        #if IN_COLAB:  # not a problem with jupyter + create print output issues\n",
        "        #    for _ in range(n_iter):\n",
        "        #        get_ipython().kernel.do_one_iteration()\n",
        "\n",
        "\n",
        "    def dl_tweets():\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_dl_tweets.button_style = 'primary'\n",
        "        ensure_widgets_updated()\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower().strip()\n",
        "        log_dl_tweets.clear_output(wait=True)\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        with log_dl_tweets:\n",
        "            try:\n",
        "                print_html(f'\\nDownloading {handle_widget.value.strip()} tweets... This should take no more than a minute!')\n",
        "                http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "                res = http.request(\"GET\", f\"http://us-central1-huggingtweets.cloudfunctions.net/get_tweets?handle={handle}&force=1\")\n",
        "                res = json.loads(res.data.decode('utf-8'))\n",
        "                \n",
        "                # save user info\n",
        "                card_val['USER_HANDLE'] = handle\n",
        "                card_val['USER_NAME'] = res['user_name']\n",
        "                card_val['USER_PROFILE'] = res['user_profile'].replace('_normal.', '_400x400.')\n",
        "                card_val['SOCIAL_LINK'] = res['social_link']\n",
        "\n",
        "                all_tweets = res['tweets']\n",
        "                curated_tweets = [fix_text(tweet) for tweet in all_tweets]\n",
        "                log_dl_tweets.clear_output(wait=True)\n",
        "                print_html(f\"\\n{res['n_tweets']} tweets from {handle_widget.value.strip()} downloaded!\\n\\n\")\n",
        "                    \n",
        "                # create dataset\n",
        "                clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "                cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
        "\n",
        "                # create a file based on multiple epochs with tweets mixed up\n",
        "                seed_data = random.randint(0,2**32-1)\n",
        "                dataRandom = random.Random(seed_data)\n",
        "                total_text = '<|endoftext|>'\n",
        "                for _ in range(EPOCHS):\n",
        "                    dataRandom.shuffle(cool_tweets)\n",
        "                    total_text += '<|endoftext|>'.join(cool_tweets) + '<|endoftext|>'\n",
        "\n",
        "                # display a few tweets\n",
        "                display(HTML(html_table([[t] for t in curated_tweets[:8]])))\n",
        "                ensure_widgets_updated()  # for auto-scroll\n",
        "                \n",
        "                if len(total_text) / EPOCHS < 6000:\n",
        "                    # need about 4000 chars for one data sample (but depends on spaces, etc)\n",
        "                    raise ValueError(f\"Error: this user does not have enough tweets to train a Neural Network\\n{res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} boring tweets... only {len(cool_tweets)} tweets kept!\")\n",
        "                if len(total_text) / EPOCHS < 40000:\n",
        "                    print_html('\\n\\n<b>Warning: this user does not have many tweets which may impact the results of the Neural Network</b>\\n\\n')\n",
        "\n",
        "                print_html('\\nCreating dataset...')\n",
        "                ensure_widgets_updated() # for auto-scroll\n",
        "                \n",
        "                # log dataset\n",
        "                with log_debug:\n",
        "                    wandb.login(key=res['wandb'])\n",
        "\n",
        "                    with wandb.init(name=f'@{handle}-dl_data',\n",
        "                                    job_type='dl_data',\n",
        "                                    config={'huggingtweets version':HW_VERSION,\n",
        "                                            'handle':handle},\n",
        "                                    project = WANDB_PROJECT,\n",
        "                                    entity = WANDB_ENTITY,\n",
        "                                    notes = WANDB_NOTES,\n",
        "                                    reinit=True) as run:\n",
        "                        # log raw tweets as input\n",
        "                        global metadata\n",
        "                        metadata={'handle':handle,\n",
        "                                  'tweets downloaded': res['n_tweets'],\n",
        "                                  'retweets': res['n_RT'],\n",
        "                                  'tweets kept': len(all_tweets),\n",
        "                                  'huggingtweets version': HW_VERSION}\n",
        "                        artifact_input = wandb.Artifact(\n",
        "                            f'tweets-{handle}',\n",
        "                            type='raw-dataset',\n",
        "                            description=f'Raw tweets from @{handle} downloaded with Tweepy',                            \n",
        "                            metadata=metadata)\n",
        "                        with artifact_input.new_file('tweets.txt') as f:\n",
        "                            json.dump(all_tweets, f, indent=0)\n",
        "                        run.log_artifact(artifact_input)\n",
        "\n",
        "                    with wandb.init(name=f'@{handle}-preprocess',\n",
        "                                    job_type='preprocess',\n",
        "                                    config={'huggingtweets version':HW_VERSION,\n",
        "                                            'handle':handle,\n",
        "                                            'seed data':seed_data},\n",
        "                                    project = WANDB_PROJECT,\n",
        "                                    entity = WANDB_ENTITY,\n",
        "                                    notes = WANDB_NOTES,\n",
        "                                    reinit=True) as run:\n",
        "                        run.use_artifact(artifact_input)\n",
        "                        # log dataset as output                        \n",
        "                        metadata={'handle':handle,\n",
        "                                  'tweets downloaded': res['n_tweets'],\n",
        "                                  'retweets': res['n_RT'],\n",
        "                                  'short tweets': len(all_tweets) - len(cool_tweets),\n",
        "                                  'tweets kept': len(cool_tweets),\n",
        "                                  'seed data': seed_data,\n",
        "                                  'epochs': EPOCHS,\n",
        "                                  'huggingtweets version': HW_VERSION}\n",
        "                        global artifact_dataset\n",
        "                        artifact_dataset = wandb.Artifact(\n",
        "                            f'dataset-{handle}',\n",
        "                            type='train-dataset',\n",
        "                            description=f'Dataset created from tweets of @{handle}',\n",
        "                            metadata=metadata)\n",
        "                        with open(f'data_{handle}_train.txt', 'w') as f:\n",
        "                            f.write(total_text)\n",
        "                        artifact_dataset.add_file(f'data_{handle}_train.txt')\n",
        "                        run.log_artifact(artifact_dataset)\n",
        "                        \n",
        "                        # keep track of url\n",
        "                        wandb_url = wandb.run.get_url()\n",
        "                        card_val['WANDB_PREPROCESS'] = wandb_url\n",
        "\n",
        "                    # Save data info\n",
        "                    card_val['TWEETS_DL'] = str(res['n_tweets'])\n",
        "                    card_val['RETWEETS'] = str(res['n_RT'])\n",
        "                    card_val['SHORT_TWEETS'] = str(len(all_tweets) - len(cool_tweets))\n",
        "                    card_val['TWEETS_KEPT'] = str(len(cool_tweets))\n",
        "                \n",
        "                success_try = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_dl_tweets.button_style = 'danger'\n",
        "        \n",
        "        if success_try:\n",
        "            run_dl_tweets.button_style = 'success'\n",
        "            log_finetune.clear_output(wait=True)\n",
        "            with log_finetune:\n",
        "                print_html('\\nFine-tune your model by clicking on \"Train Neural Network\"')\n",
        "            run_finetune.disabled = False\n",
        "            with log_dl_tweets:\n",
        "                print_html(f\"\\nðŸŽ‰ Dataset created: {res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} short tweets... keeping {len(cool_tweets)} tweets\")\n",
        "\n",
        "        handle_widget.disabled = False\n",
        "        run_dl_tweets.disabled = False\n",
        "                \n",
        "    handle_widget = widgets.Text(value='@elonmusk',\n",
        "                                placeholder='Enter twitter handle')\n",
        "\n",
        "    run_dl_tweets = widgets.Button(\n",
        "        description='Download tweets',\n",
        "        button_style='primary')\n",
        "    def on_run_dl_tweets_clicked(b):\n",
        "        dl_tweets()\n",
        "    run_dl_tweets.on_click(on_run_dl_tweets_clicked)\n",
        "\n",
        "    log_restart = widgets.Output()\n",
        "    log_dl_tweets = widgets.Output()\n",
        "    \n",
        "    def finetune():\n",
        "        # transformers imports later as wandb needs to have logged in\n",
        "        import transformers\n",
        "        from transformers import (\n",
        "            AutoTokenizer, AutoModelForCausalLM,\n",
        "            TextDataset, DataCollatorForLanguageModeling,\n",
        "            Trainer, TrainingArguments,\n",
        "            get_cosine_schedule_with_warmup)\n",
        "        from transformers.hf_api import HfApi\n",
        "\n",
        "        if run_finetune.button_style == 'success':\n",
        "            # user double clicked before start of function\n",
        "            return\n",
        "\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_finetune.disabled = True\n",
        "        run_finetune.button_style = 'primary'\n",
        "\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower().strip()\n",
        "        model_url = f'https://huggingface.co/huggingtweets/{handle}'\n",
        "        log_finetune.clear_output(wait=True)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        with log_finetune:\n",
        "            print_html(f'\\nTraining Neural Network on {handle_widget.value.strip()} tweets... This could take up to 2-3 minutes!\\n')\n",
        "            progress = widgets.FloatProgress(value=0.1, min=0.0, max=1.0, bar_style = 'info')\n",
        "            label_progress = widgets.Label('Downloading pre-trained neural network...')\n",
        "            display(widgets.HBox([progress, label_progress]))\n",
        "\n",
        "        with log_debug:\n",
        "            try:                \n",
        "                # Setting up pre-trained neural network\n",
        "                global trainer\n",
        "                tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "                model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=pathlib.Path('cache').resolve())\n",
        "                block_size = tokenizer.model_max_length\n",
        "                train_dataset = TextDataset(tokenizer=tokenizer, file_path=f'data_{handle}_train.txt', block_size=block_size, overwrite_cache=True)\n",
        "                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "                seed = random.randint(0,2**32-1)\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=f'output/{handle}',\n",
        "                    overwrite_output_dir=True,\n",
        "                    do_train=True,\n",
        "                    num_train_epochs=1,\n",
        "                    per_device_train_batch_size=1,\n",
        "                    prediction_loss_only=True,\n",
        "                    logging_steps=5,\n",
        "                    save_steps=0,\n",
        "                    seed=seed,\n",
        "                    learning_rate = LEARNING_RATE)\n",
        "                \n",
        "                # create wandb run (before it's done automatically by Trainer)\n",
        "                combined_dict = {**model.config.to_dict(), **training_args.to_sanitized_dict()}\n",
        "                run = wandb.init(name=f'@{handle}-train',\n",
        "                                 job_type='train',\n",
        "                                 config={'huggingtweets version':HW_VERSION,\n",
        "                                         'pytorch version': torch.__version__,\n",
        "                                         'transformers version': transformers.__version__,\n",
        "                                         'handle':handle,\n",
        "                                         **combined_dict},\n",
        "                                 project = WANDB_PROJECT,\n",
        "                                 entity = WANDB_ENTITY,\n",
        "                                 notes = WANDB_NOTES,\n",
        "                                 reinit=True)\n",
        "                \n",
        "                # keep track of url\n",
        "                wandb_url = wandb.run.get_url()\n",
        "                card_val['WANDB_TRAIN'] = wandb_url\n",
        "\n",
        "                # Set-up Trainer\n",
        "                os.environ['WANDB_WATCH'] = 'false'  # used in Trainer\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    args=training_args,\n",
        "                    data_collator=data_collator,\n",
        "                    train_dataset=train_dataset)\n",
        "                \n",
        "                # Update lr scheduler\n",
        "                train_dataloader = trainer.get_train_dataloader()\n",
        "                num_train_steps = len(train_dataloader)\n",
        "                trainer.create_optimizer_and_scheduler(num_train_steps)\n",
        "                trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "                    trainer.optimizer,\n",
        "                    num_warmup_steps=0,\n",
        "                    num_training_steps=num_train_steps)\n",
        "\n",
        "                progress.value = 0.3\n",
        "                label_progress.value = 'Logging input artifacts to W&B...'\n",
        "\n",
        "                # log dataset and pretrained model\n",
        "                run.use_artifact(artifact_dataset)\n",
        "                artifact_gpt2 = wandb.Artifact(\n",
        "                    f'gpt2',\n",
        "                    type='pretrained-model',\n",
        "                    description=f'Pretrained model from OpenAI downloaded from ðŸ¤— Transformers: https://huggingface.co/gpt2',\n",
        "                    metadata={'huggingtweets version': HW_VERSION})\n",
        "                artifact_gpt2.add_dir('cache', name='gpt2')\n",
        "                run.use_artifact(artifact_gpt2)\n",
        "                progress.value = 0.4\n",
        "                label_progress.value = 'Training neural network...'\n",
        "                \n",
        "                p_start, p_end = 0.4, 0.8\n",
        "                def progressify(f):\n",
        "                    \"Control progress bar when calling f\"\n",
        "                    def inner(*args, **kwargs):\n",
        "                        if trainer.state.epoch is not None:\n",
        "                            # we only have one epoch, EPOCHS is built into dataset\n",
        "                            progress.value = p_start + trainer.state.epoch * (p_end - p_start)\n",
        "                        return f(*args, **kwargs)\n",
        "                    return inner\n",
        "        \n",
        "                trainer.training_step = progressify(trainer.training_step)\n",
        "                \n",
        "                # Training neural network\n",
        "                with log_finetune:\n",
        "                    display(wandb.jupyter.Run())\n",
        "                    print_html('\\n')\n",
        "                    display(widgets.HBox([progress, label_progress]))\n",
        "                trainer.train()\n",
        "\n",
        "                # set model config parameters\n",
        "                trainer.model.config.task_specific_params['text-generation'] = {\n",
        "                    'do_sample': True,\n",
        "                    'min_length': 10,\n",
        "                    'max_length': 160,\n",
        "                    'temperature': 1.,\n",
        "                    'top_p': 0.95,\n",
        "                    'prefix': '<|endoftext|>'}\n",
        "                \n",
        "                # create model repo\n",
        "                label_progress.value = 'Setting up Hugging Face model repo'\n",
        "                model_name = handle\n",
        "                shutil.rmtree(model_name, ignore_errors=True)\n",
        "                model_path = pathlib.Path(model_name)\n",
        "                try:\n",
        "                    global hfapi, token, namespace\n",
        "                    hfapi = HfApi()\n",
        "                    user, namespace = 'huggingtweets-app', 'huggingtweets'\n",
        "                    token = hfapi.login(user, namespace)\n",
        "                    assert hfapi.whoami(token)[0] == user, \"Could not log into Hugging Face\"\n",
        "                    try:\n",
        "                        url = hfapi.create_repo(token, name=model_name, organization=namespace)\n",
        "                    except:\n",
        "                        # TODO: use exist_ok=True to avoid try loop\n",
        "                        pass\n",
        "                    !git clone https://$token@huggingface.co/huggingtweets/$model_name\n",
        "                \n",
        "                except:\n",
        "                    with log_finetune:\n",
        "                        print_html(f'\\n<b>Could not create a model repo</b>\\n{e}')\n",
        "                # remove non-git files\n",
        "                for f in pathlib.Path(model_name).glob('*'):\n",
        "                    if f.suffix:\n",
        "                        f.unlink()\n",
        "\n",
        "                # save new model files\n",
        "                trainer.save_model(model_name)\n",
        "                \n",
        "                # log model to huggingface\n",
        "                label_progress.value = 'Committing model to Hugging Face'\n",
        "                hf_urls = []\n",
        "                try:\n",
        "                    create_model_card(card_val, model_name)\n",
        "\n",
        "                    # upload files                    \n",
        "                    !git config --global user.email \"boris.dayma@gmail.com\"\n",
        "                    %cd $model_name\n",
        "                    !git add .\n",
        "                    !git commit -am \"Update of model\"\n",
        "                    !git push\n",
        "                    %cd ..\n",
        "\n",
        "\n",
        "                    '''assert model_path.is_dir(), f\"Expected {model_path} to be a directory\"\n",
        "                    files = [(str(f.resolve()), str(f)) for f in model_path.glob('*')]\n",
        "                    assert len(files) == 7, f\"Unexpected number of files in model directory: {len(files)}\"\n",
        "                    for filepath, filename in files:\n",
        "                        hf_urls.append(hfapi.presign_and_upload(token, filename=filename, filepath=filepath, organization=namespace))\n",
        "                        progress.value += 0.02'''\n",
        "                \n",
        "                except Exception as e:\n",
        "                    with log_finetune:\n",
        "                        print_html(f'\\n<b>Could not upload the model to Hugging Face</b>\\n{e}')\n",
        "\n",
        "                # log model to W&B\n",
        "                label_progress.value = 'Logging model to W&B...'\n",
        "                global metadata\n",
        "                metadata={'model url':model_url,\n",
        "                          'seed trainer':seed,\n",
        "                          **metadata}\n",
        "                artifact_trained = wandb.Artifact(\n",
        "                    model_name,\n",
        "                    type='finetuned-model',\n",
        "                    description=f'Model fine-tuned on tweets from @{handle}',\n",
        "                    metadata=metadata)\n",
        "                for hf_url in hf_urls:\n",
        "                    artifact_trained.add_reference(hf_url)\n",
        "                run.log_artifact(artifact_trained)\n",
        "                progress.value = 0.98\n",
        "\n",
        "                run_finetune.button_style = 'success'\n",
        "                run_predictions.disabled = False\n",
        "\n",
        "                progress.value = 1.0\n",
        "                progress.bar_style = 'success'\n",
        "                success_try = True\n",
        "\n",
        "                label_progress.value = 'ðŸŽ‰ Neural network trained successfully!'\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                with log_predictions:\n",
        "                    print_html('\\nEnter the start of a sentence and click \"Run predictions\"')\n",
        "                with log_restart:\n",
        "                    print_html('\\n<b>To change user, refresh the page</b>\\n')\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_finetune.button_style = 'danger'\n",
        "                run_finetune.disabled = False\n",
        "                            \n",
        "        if not success_try:\n",
        "            display(log_debug)\n",
        "            progress.bar_style = 'danger'\n",
        "        \n",
        "    run_finetune = widgets.Button(\n",
        "        description='Train Neural Network',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_finetune_clicked(b):\n",
        "        finetune()\n",
        "    run_finetune.on_click(on_run_finetune_clicked)\n",
        "\n",
        "    log_finetune = widgets.Output()\n",
        "    with log_finetune:\n",
        "        print_html('\\nWaiting for Step 1 to complete...')\n",
        "\n",
        "    predictions = []\n",
        "    \n",
        "    def shorten_text(text, max_char):\n",
        "        while len(text) > max_char:\n",
        "            text = ' '.join(text.split()[:-1]) + 'â€¦'\n",
        "        return text\n",
        "        \n",
        "    def predict():\n",
        "        run_predictions.disabled = True\n",
        "        start_widget.disabled = True\n",
        "        run_predictions.button_style = 'primary'\n",
        "        global handle\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle_uncased = handle.strip()\n",
        "        handle = handle.lower().strip()\n",
        "        model_url = f'https://huggingface.co/huggingtweets/{handle}'\n",
        "        log_predictions.clear_output(wait=True)\n",
        "\n",
        "        # tweet buttons don't appear well in colab if within log_predictions widget\n",
        "        # we reset the entire cell\n",
        "        clear_output(wait=True)\n",
        "        display(widgets.VBox([start_widget, run_predictions, log_model, log_predictions]))\n",
        "        stylize()\n",
        "\n",
        "        def tweet_html(tweet_text, tweet_url):\n",
        "            tweet_text = shorten_text(tweet_text, 238)\n",
        "            tweet_text = tweet_text.replace('\"', '&quot;')\n",
        "\n",
        "            return '<div style=\"padding-top: 4px\"><a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" data-size=\"large\" '\\\n",
        "                    f'data-text=\"{tweet_text}\" '\\\n",
        "                    f'data-url=\"{tweet_url}\" data-related=\"borisdayma,weights_biases,huggingface\"'\\\n",
        "                    'data-show-count=\"false\">Tweet</a></div><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>'\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        # get start sentence\n",
        "        ensure_widgets_updated()\n",
        "        start = start_widget.value.strip()\n",
        "                \n",
        "        with log_predictions:\n",
        "            print_html(f'\\nPerforming predictions of @{handle} starting with \"{start}\"...\\nThis should take no more than 10 seconds!')\n",
        "        \n",
        "        with log_debug:\n",
        "            try:\n",
        "                # start a wandb run (should never happen)\n",
        "                if wandb.run is None:\n",
        "                    print('Unexpected missing W&B run process')\n",
        "                    wandb.init()\n",
        "                \n",
        "                # prepare input\n",
        "                start_with_bos = '<|endoftext|>' + start\n",
        "                encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "                encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
        "\n",
        "                # prediction\n",
        "                output_sequences = trainer.model.generate(\n",
        "                    input_ids=encoded_prompt,\n",
        "                    max_length=160,\n",
        "                    min_length=10,\n",
        "                    temperature=1.,\n",
        "                    top_p=0.95,\n",
        "                    do_sample=True,\n",
        "                    num_return_sequences=10\n",
        "                    )\n",
        "                generated_sequences = []\n",
        "\n",
        "                # decode prediction\n",
        "                for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "                    generated_sequence = generated_sequence.tolist()\n",
        "                    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "                    if not ALLOW_NEW_LINES:\n",
        "                        limit = text.find('\\n')\n",
        "                        text = text[: limit if limit != -1 else None]\n",
        "                    generated_sequences.append(text.strip())\n",
        "                \n",
        "                for i, g in enumerate(generated_sequences):\n",
        "                    predictions.append([start, g])\n",
        "\n",
        "                # create previews\n",
        "                r = requests.post('https://us-central1-huggingtweets.cloudfunctions.net/screenshot',\n",
        "                                  data = {\"NAME\": card_val['USER_NAME'],\n",
        "                                          \"HANDLE\": card_val['USER_HANDLE'],\n",
        "                                          \"URL\": card_val['USER_PROFILE'],\n",
        "                                          \"INPUT\": start,\n",
        "                                          \"OUTPUTS\": generated_sequences})\n",
        "                ids = r.json()\n",
        "                global model_preview\n",
        "                global hfapi, token, namespace\n",
        "                if model_preview is None:\n",
        "                    model_preview = f'https://www.huggingtweets.com/{handle}/{ids[0]}/predictions.png'\n",
        "                    card_val['SOCIAL_LINK'] = model_preview\n",
        "                    create_model_card(card_val, handle)\n",
        "                    readme = pathlib.Path(handle) / 'README.md'\n",
        "                    readme_path, readme_name = str(readme.resolve()), str(readme)\n",
        "                    hfapi.presign_and_upload(token, filename=readme_name, filepath=readme_path, organization=namespace)\n",
        "                    with log_model:\n",
        "                        print_html(\"<h2>Model Preview (select a tweet to update)</h2>\")\n",
        "                        display(HTML(f'<img src=\"{model_preview}\" width=560 style=\"border: 1px solid lightgray; margin:5px;\">'))\n",
        "\n",
        "                # log predictions\n",
        "                wandb.log({'examples': wandb.Table(data=predictions, columns=['Input', 'Prediction'])})\n",
        "\n",
        "                # display tweets\n",
        "                widgets_tweet = []\n",
        "                center = widgets.Layout(align_items='center', display='flex')\n",
        "                layout_twitter = widgets.Layout(width = '76px')\n",
        "                for i, (g, id) in enumerate(zip(generated_sequences, ids)):\n",
        "                    preview_button = create_button(id)\n",
        "                    tweet_button = tweet_html(f'I love this tweet generated by my AI bot of @{handle_uncased} with huggingtweets!\\nPlay with my model or create your own!\\n\\nMade by @borisdayma using @huggingface and @weights_biases',\n",
        "                                              f'http://www.huggingtweets.com/{handle}/{id}/predictions.html')\n",
        "                    w = widgets.HBox([preview_button,\n",
        "                                      widgets.HTML(tweet_button, layout=layout_twitter),\n",
        "                                      widgets.HTML(g)],\n",
        "                                     layout=center)\n",
        "                    w.add_class(\"table_odd\" if i%2 else \"table_even\")\n",
        "                    widgets_tweet.append(w)\n",
        "\n",
        "                # make model share table\n",
        "                tweet_share = f'I created an AI bot of @{handle_uncased} with huggingtweets!\\nPlay with my model or create your own!\\n\\nMade by @borisdayma using @huggingface and @weights_biases'\n",
        "                link_model = f'<a href=\"{model_url}\" rel=\"noopener\" target=\"_blank\">{model_url}</a>'\n",
        "                share_data = [[tweet_html(tweet_share, model_url),\n",
        "                               f'ðŸŽ‰ Share @{handle_uncased} model: {link_model} <i>(may take 30 seconds to become active)</i>']]\n",
        "                share_table = HTML(html_table(share_data))\n",
        "\n",
        "                run_predictions.button_style = 'success'\n",
        "                success_try = True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_predictions.button_style = 'danger'\n",
        "\n",
        "        if success_try:\n",
        "            with log_predictions:\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                \n",
        "                # twitter button does not update within widget in colab\n",
        "                if not IN_COLAB:\n",
        "                    print_html('\\n')\n",
        "                    display(share_table)\n",
        "                    print_html('\\n<b>Share your model and favorite tweets or try new predictions!\\nTwitter will display the image (reload the tweet to preview)!</b>\\n\\n')\n",
        "                    for w in widgets_tweet:\n",
        "                        display(w)\n",
        "                    print_html('\\n<b>Share your model and favorite tweets or try new predictions!\\nTwitter will display the image (reload the tweet to preview)!</b>\\n\\n')\n",
        "\n",
        "            if IN_COLAB:\n",
        "                print_html('\\n')\n",
        "                display(share_table)\n",
        "                print_html('\\n<b>Share your model and favorite tweets or try new predictions!\\nTwitter will display the image (reload the tweet to preview)!</b>\\n\\n')\n",
        "                for w in widgets_tweet:\n",
        "                    display(w)\n",
        "                print_html('\\n<b>Share your model and favorite tweets or try new predictions!\\nTwitter will display the image (reload the tweet to preview)!</b>\\n\\n')\n",
        "        else:\n",
        "            display(log_debug)\n",
        "        \n",
        "        run_predictions.disabled = False\n",
        "        start_widget.disabled = False\n",
        "                \n",
        "    start_widget = widgets.Text(value='My dream is',\n",
        "                                placeholder='Start a sentence')\n",
        "\n",
        "    run_predictions = widgets.Button(\n",
        "        description='Run predictions',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_predictions_clicked(b):\n",
        "        predict()\n",
        "    run_predictions.on_click(on_run_predictions_clicked)\n",
        "\n",
        "    log_predictions = widgets.Output()\n",
        "    with log_predictions:\n",
        "        print_html('\\nWaiting for Step 2 to complete...')\n",
        "    log_model = widgets.Output()\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print_html(\"ðŸŽ‰ Environment set-up correctly! You're ready to move to Step 1!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpSPr0T8AHD"
      },
      "source": [
        "## Step 1 - Enter a Twitter handle\n",
        "\n",
        "Enter a Twitter handle and click Download tweets. This gives the model a dataset of examples to train on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "6O-8Kr_m8AHE"
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "stylize()\n",
        "display(widgets.VBox([handle_widget, run_dl_tweets, log_restart, log_dl_tweets]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_ArgCZ8AHH"
      },
      "source": [
        "## Step 2 - Train your Neural Network\n",
        "\n",
        "Fine-tune a language model on your unique set of tweets to generate predictions.\n",
        "\n",
        "The model is downloaded from [HuggingFace transformers](https://huggingface.co/), an awesome open source library for Natural Language Processing and training is logged through [Weights & Biases](http://docs.wandb.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "CpxBQYF88AHJ"
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "stylize()\n",
        "display(widgets.VBox([run_finetune, log_finetune]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfRPh4V18AHM"
      },
      "source": [
        "## Step 3: Generate tweets\n",
        "\n",
        "Type the beginning of a tweet, press Run predictions, and the model will try to come up with a realistic ending to your tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FlMACi0-8AHN"
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "stylize()\n",
        "if IN_COLAB:\n",
        "    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 2000})'''))\n",
        "display(widgets.VBox([start_widget, run_predictions, log_model, log_predictions]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlRI2hsBKtz6"
      },
      "source": [
        "Huggingtweets is still in its infancy and will get better over time!\n",
        "\n",
        "In the future, it will train continuously to become a Twitter expert!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iawnVxrItM"
      },
      "source": [
        "## About\n",
        "\n",
        "*Built by Boris Dayma*\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n",
        "\n",
        "My main goals with this project are:\n",
        "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
        "* to make AI accessible to everyone ;\n",
        "* to have fun!\n",
        "\n",
        "For more details, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
        "\n",
        "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**\n",
        "\n",
        "## Resources\n",
        "\n",
        "* [Explore the W&B report](https://app.wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-model-to-generate-tweets--VmlldzoxMTY5MjI) to understand how the model works\n",
        "* [HuggingFace and W&B integration documentation](https://docs.wandb.com/library/integrations/huggingface)\n",
        "\n",
        "## Got questions about W&B?\n",
        "\n",
        "If you have any questions about using W&B to track your model performance and predictions, please reach out to the [slack community](http://bit.ly/wandb-forum)."
      ]
    }
  ]
}